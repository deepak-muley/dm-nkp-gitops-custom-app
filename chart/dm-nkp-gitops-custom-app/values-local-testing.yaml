# Local Testing Values - For use with OpenTelemetry Operator and kube-prometheus-stack
# This is used when deploying locally for testing with OpenTelemetry Operator
#
# Usage:
#   1. Deploy OpenTelemetry Operator (for local testing):
#      kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
#      OR via Helm:
#      helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
#      helm upgrade --install opentelemetry-operator open-telemetry/opentelemetry-operator \
#        --namespace opentelemetry --create-namespace
#
#   2. Create OpenTelemetryCollector CR (or use default collector):
#      kubectl apply -f - <<EOF
#      apiVersion: opentelemetry.io/v1alpha1
#      kind: OpenTelemetryCollector
#      metadata:
#        name: collector
#        namespace: opentelemetry
#      spec:
#        mode: deployment
#        config: |
#          receivers:
#            otlp:
#              protocols:
#                grpc:
#                  endpoint: 0.0.0.0:4317
#          exporters:
#            prometheus:
#              endpoint: 0.0.0.0:8889
#            logging:
#          service:
#            pipelines:
#              metrics:
#                receivers: [otlp]
#                exporters: [prometheus]
#      EOF
#
#   3. Deploy kube-prometheus-stack (for Prometheus/Grafana):
#      helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
#      helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
#        --namespace monitoring --create-namespace
#
#   4. Deploy application with these values:
#      helm upgrade --install dm-nkp-gitops-custom-app . \
#        --namespace default -f values-local-testing.yaml

# Image configuration (local testing)
image:
  repository: dm-nkp-gitops-custom-app
  tag: "test"  # Local testing image
  pullPolicy: Never  # For kind/minikube

# OpenTelemetry Configuration - References locally deployed OpenTelemetry Operator
# OpenTelemetry Operator is installed in 'opentelemetry' namespace
# ⚠️ IMPORTANT: Service name matches the OpenTelemetryCollector CR name (not Deployment name)
#   - Service name = CR name exactly (e.g., if CR is 'collector', service is 'collector')
#   - Deployment name = CR name + "-collector" suffix (e.g., 'collector-collector')
# ⚠️ Verify service name: kubectl get svc -n opentelemetry | grep collector
#   Check CR name: kubectl get opentelemetrycollector -n opentelemetry
opentelemetry:
  enabled: true
  collector:
    # Local OpenTelemetry Operator collector endpoint
    # ⚠️ IMPORTANT: Service name pattern is <CR-name>-collector (not just CR name)
    # For e2e-demo-otel.sh: CR is named 'otel-collector', so service is 'otel-collector-collector'
    endpoint: "otel-collector-collector.observability.svc.cluster.local:4317"
    # Alternative common names (if CR is named differently):
    # If CR is 'collector': endpoint: "collector-collector.opentelemetry.svc.cluster.local:4317"
    # If CR is 'otelcol': endpoint: "otelcol-collector.opentelemetry.svc.cluster.local:4317"
  service:
    name: "dm-nkp-gitops-custom-app"
    namespace: ""
  env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "otel-collector-collector.observability.svc.cluster.local:4317"
      # ⚠️ IMPORTANT: Service name pattern is <CR-name>-collector
      # Alternative (update based on your OpenTelemetryCollector CR name):
      # If CR is 'collector': value: "collector-collector.opentelemetry.svc.cluster.local:4317"
    - name: OTEL_SERVICE_NAME
      value: "dm-nkp-gitops-custom-app"
    - name: OTEL_RESOURCE_ATTRIBUTES
      value: "service.name=dm-nkp-gitops-custom-app,service.version=0.1.0,environment=local"
    - name: OTEL_EXPORTER_OTLP_INSECURE
      value: "true"
    - name: OTEL_LOGS_ENABLED
      value: "true"  # Enable OTLP logging

# Monitoring Configuration - References locally deployed kube-prometheus-stack
# ⚠️ IMPORTANT: ServiceMonitor will be deployed to Release.Namespace by default
#   If Prometheus Operator is in a different namespace, set the namespace value
#   Make sure the Prometheus Operator can discover ServiceMonitors in that namespace
monitoring:
  serviceMonitor:
    enabled: true
    # Use Release.Namespace for local testing (set to "monitoring" if Prometheus Operator requires it)
    namespace: ""  # Empty = Release.Namespace (default), or set to "monitoring" if Prometheus Operator is there
    interval: 30s
    scrapeTimeout: 10s
    otelCollector:
      # OpenTelemetry Operator uses standard labels (same as NKP)
      selectorLabels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/component: collector
      namespace: "opentelemetry"  # OpenTelemetry Operator namespace
      prometheusPort: "otlp"  # Common port name for OTel Operator collector (verify)
      # Alternative: "prometheus" if collector exposes Prometheus metrics on separate port
      prometheusPath: "/metrics"

# Gateway API Configuration - Optional for local testing
# Only enable if Traefik with Gateway API is installed in the test cluster
# Gateway API CRDs: Install via: kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml
gateway:
  enabled: false  # Disabled by default for local testing (can enable if Gateway API is installed)
  parentRef:
    name: "traefik"
    namespace: "traefik-system"
  hostnames:
    - "dm-nkp-gitops-custom-app.local"

# TLS Configuration - DISABLED for local testing
# ⚠️ Let's Encrypt certificates DO NOT work in local clusters because:
#    1. Local domains (*.local) are not publicly resolvable
#    2. ACME HTTP-01 challenge requires Let's Encrypt servers to reach your cluster
#    3. Local clusters are not accessible from the internet
# TLS/HTTPS is only for production deployments with real public domains
tls:
  enabled: false  # MUST be false for local testing
  clusterIssuer:
    create: false  # Don't create ClusterIssuer in local testing
  certificate:
    create: false  # Don't create Certificate in local testing

# Grafana Dashboard Configuration - References locally deployed kube-prometheus-stack Grafana
# ⚠️ IMPORTANT: For local testing, Grafana dashboards will be deployed to Release.Namespace by default
#   If kube-prometheus-stack is deployed in "monitoring" namespace, you can:
#   1. Create the monitoring namespace first: kubectl create namespace monitoring
#   2. Or deploy kube-prometheus-stack to the same namespace as the app
#   3. Or set namespace: "" to use Release.Namespace
grafana:
  dashboards:
    enabled: true
    # For e2e-demo-otel.sh: Use observability namespace where Grafana is deployed
    # This will be overridden by --set flag in the script, but set a sensible default
    namespace: "observability"  # Default to observability namespace for local testing
    folder: "/"  # Root folder for local testing
    # Enable all dashboards for local testing
    platformFootprint:
      enabled: true  # Enable platform footprint dashboard
      folder: "/"  # Use root folder for local testing
    clusterFootprint:
      enabled: true  # Enable cluster footprint dashboard
      folder: "/"  # Use root folder for local testing
  
  # Datasources Configuration - Enabled for local testing via ConfigMap provisioning
  # Note: For kube-prometheus-stack, datasources can be configured via ConfigMaps with label grafana_datasource=1
  # The ConfigMap will be auto-discovered by Grafana if sidecar.datasources.enabled=true (default in kube-prometheus-stack)
  datasources:
    enabled: true  # Enabled to provision datasources via ConfigMap (persistent, survives restarts)
    namespace: "observability"  # Default to observability namespace for local testing
    prometheus:
      enabled: true
      # kube-prometheus-stack service name pattern: <release-name>-kube-prometheus-prometheus
      # Common release name: "prometheus" for local testing
      url: "http://prometheus-kube-prometheus-prometheus.observability.svc.cluster.local:9090"
      # Alternative (if release name is different):
      # url: "http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090"
      isDefault: true
    loki:
      enabled: true
      # Update URL dynamically - will be set by e2e script based on detected Loki service
      # Default to loki-distributed gateway (common for local testing)
      url: "http://loki-loki-distributed-gateway.observability.svc.cluster.local:80"
      # Alternative patterns (will be auto-detected by e2e script):
      # If using loki-simple-scalable: url: "http://loki.observability.svc.cluster.local:3100"
      # If using loki-distributed: url: "http://loki-loki-distributed-gateway.observability.svc.cluster.local:80"
    tempo:
      enabled: true
      # Update URL dynamically - will be set by e2e script based on detected Tempo service
      # Default Tempo service name
      url: "http://tempo.observability.svc.cluster.local:3200"

# Resource limits for local testing (smaller)
resources:
  limits:
    cpu: 200m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi

# No autoscaling for local testing
autoscaling:
  enabled: false
